# Study Log
Checklist to help me keep track of milestones in a study plan designed to help familiarize myself with the fundementals of ML (particularly deep learning) theory. Will probably be expanded to cover other domains of knowledge in software engineering.

Inspiration for this log: https://github.com/amitness/learning

# AI Study Plan Resources Checklist

## Foundations
**Purpose**: Build a strong foundation in Linear Algebra, Calculus, Statistics, Neural Networks, and AI frameworks.

### Foundational Mathematics Resources

|Resource|Progress|Notes|
|---|---|---|
|[Imperial College London, MOOC: Mathematics for Machine Learning - Specialization, Linear Algebra](https://www.coursera.org/learn/linear-algebra-machine-learning/)|✅|
|[Imperial College London, MOOC: Mathematics for Machine Learning - Specialization, Multivariant Calculus](https://www.coursera.org/specializations/mathematics-machine-learning)|✅|
|[Imperial College London, MOOC: Mathematics for Machine Learning - Specialization, Linear Algebra](https://www.coursera.org/specializations/mathematics-machine-learning)|⬜|
|MIT OpenCourseWare: [Linear Algebra Full Course](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/) |⬜ |for deepening knowledge/review|

### Programming Resources and General Tools/Setup
|Resource|Progress|
|---|---|
|Google Colab for experimentation: [Colab](https://colab.research.google.com/)|⬜|
|[PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)|⬜|


### Machine Learning/Neural Networks Foundational Resources
|Resources|Progress|Notes|
|---|---|---|
|[LinkedIn Learning: Building Computer Vision Applications with Python](https://www.linkedin.com/learning/building-computer-vision-applications-with-python/computer-vision-under-the-hood)|✅|turned out to be more of a graphics processing course, but at least provides a good intro to convolution filters (just does not actually provide the whole picture with regard to convolutional neural network architecture)|
|[Andrew Ng: Neural Networks and Deep Learning](https://www.coursera.org/learn/neural-networks-deep-learning?specialization=deep-learning)|✅||
|[Andrew Ng: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network?specialization=deep-learning)|✅||
|[Andrew Ng: Structuring Machine Learning Projects](https://www.coursera.org/learn/machine-learning-projects?specialization=deep-learning)|✅| |
|[Andrew Ng: Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks?specialization=deep-learning)|✅| |
|[Andrew Ng: Sequence Models](https://www.coursera.org/learn/nlp-sequence-models?specialization=deep-learning)|✅|Found this sequence of deep learning courses overall extremely illuminating. Definititely leaves you with enough of an understanding of LSTMs/GRUs, ConvNets, and ResNets to start tinkering confidently. Final module on tranformers felt pretty abstract/rushed compared to every other module.|
|[3Blue1Brown: Transformers (how LLMs work) explained visually](https://www.youtube.com/watch?v=wjZofJX0v4M)|✅| Youtube video. Useful followup for final module of Andrew Ng's "Sequence Models" course. |
|[Andrej Karpathy: Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)|⬜|youtube video|
|[Neural machine translation with a Transformer and Keras](https://www.tensorflow.org/text/tutorials/transformer)|⬜| article from tensorflow docs|
|[Fine-tune a pretrained (transformer) model](https://huggingface.co/docs/transformers/training)|⬜|huggingface docs|
|[Textbook: *Deep Learning* by Goodfellow et al. (online version)](https://www.deeplearningbook.org/) |⬜|textbook|

### Topics to follow up on/review from ML courses:
- Understanding the properties/behavior of commonly used loss functions
- Transformer architecture
- Math review exercise: walk through backprop for a Sequential model with GRUs  or LSTM cells

---

## ML Papers

|Resource|Notes/Progress|
|---|---|
|[Attention is all you need](https://arxiv.org/abs/1706.03762)|⬜ (Introduced transformer architecture)|
|[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&_hsenc=p2ANqtz-9kgISsDFvuO8MZlBnFosRC4C4FiNqno6ahMESpHrnRkOKvDeon1AkJ43ZnkA-hwbA6vq6q)|⬜|
|[LIPNET: END-TO-END SENTENCE-LEVEL LIPREADING](https://arxiv.org/pdf/1611.01599)|⬜ Authors: Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, & Nando de Freitas|

---


### Open Source and Community
|Resource|Progress|
|---|---|
|Kaggle Datasets: [Kaggle Datasets](https://www.kaggle.com/datasets)|⬜|
---


# System Design Study Plan

## General Resources
|Resource|Notes/Progress|
|---|---|
|[Designing Data Intensive Applications](https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/) |⬜|